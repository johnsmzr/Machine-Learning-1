{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 2\n",
    "\n",
    "In this homework, we will train neural networks on the Breast Cancer dataset. For this, we will use of the Pytorch library. We will also make use of scikit-learn for the ML baselines. A first part of the homework will analyze the parameters of the network before and after training. A second part of the homework will test some regularization penalties and their effect on the generalization error.\n",
    "\n",
    "## Breast Cancer Dataset\n",
    "\n",
    "The following code extracts the Breast cancer dataset in a way that is already partitioned into training and test data. The data is normalized such that each dimension has mean 0 and variance 1. To test the robustness of our learning models, we also artificially inject 4% of mislabelings in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T15:53:50.227935Z",
     "start_time": "2022-02-28T15:53:48.099424Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.breast_cancer()\n",
    "\n",
    "nx = Xtrain.shape[1]\n",
    "nh = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "In this homework, we consider the same architecture as the one considered in Exercise 2 of the theoretical part. The class `NeuralNetworkClassifier` implements this network. The function `reg` is a regularizer which we set initially to zero (i.e. no regularizer). Because the dataset is small, the network can be optimized in batch mode, using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T15:53:53.954115Z",
     "start_time": "2022-02-28T15:53:53.403193Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy,torch,sklearn,sklearn.metrics\n",
    "from torch import nn,optim\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(nx,nh),nn.ReLU())\n",
    "        with torch.no_grad(): list(self.model)[0].weight *= 0.1\n",
    "        self.s = torch.zeros([100]); self.s[:50] = 1; self.s[50:] = -1\n",
    "        self.pool  = lambda y: y.matmul(self.s)\n",
    "        self.loss  = lambda y,t: torch.clamp(1-y*t,min=0).mean()\n",
    "\n",
    "    def reg(self): return 0\n",
    "        \n",
    "    def fit(self,X,T,nbit=10000):\n",
    "        \n",
    "        X = torch.Tensor(X)\n",
    "        T = torch.Tensor(T)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr=0.01)\n",
    "        for _ in range(nbit):\n",
    "            optimizer.zero_grad()\n",
    "            (self.loss(self.pool(self.model(X)),T)+self.reg()).backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.pool(self.model(torch.Tensor(X)))\n",
    "\n",
    "    def score(self,X,T):\n",
    "        Y = numpy.sign(self.predict(X).data.numpy())\n",
    "        return sklearn.metrics.accuracy_score(T,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Performance vs. Baselines\n",
    "\n",
    "We compare the performance of the neural network on the Breast cancer data to two other classifiers: a random forest and a support vector classification model with RBF kernel. We use the scikit-learn implementation of these models, with their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T15:54:12.540765Z",
     "start_time": "2022-02-28T15:54:04.877403Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble,svm\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(random_state=0)\n",
    "rfc.fit(Xtrain,Ttrain)\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnc = NeuralNetworkClassifier()\n",
    "nnc.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T15:54:12.587775Z",
     "start_time": "2022-02-28T15:54:12.542766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | Train Acc:  1.000 | Test Acc:  0.940\n",
      ">        SVC | Train Acc:  0.958 | Test Acc:  0.951\n",
      ">         NN | Train Acc:  1.000 | Test Acc:  0.884\n"
     ]
    }
   ],
   "source": [
    "def pretty(name,model):\n",
    "    return '> %10s | Train Acc: %6.3f | Test Acc: %6.3f'%(name,model.score(Xtrain,Ttrain),model.score(Xtest,Ttest))\n",
    "\n",
    "print(pretty('RForest',rfc))\n",
    "print(pretty('SVC',svc))\n",
    "print(pretty('NN',nnc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network performs not as good as the baselines. Most likely, the neural network has overfitted its decision boundary, in particular, on the mislabeled training examples.\n",
    "\n",
    "## Gradient, and Parameter Norms (25 P)\n",
    "\n",
    "For the model to generalize better, we assume that the gradient of the decision function should be prevented from becoming too large. Because the gradient can only be evaluated on the current data distribution (and may not generalize outside the data), we resort to the following inequality we have proven in the theoretical section for this class of neural network models:\n",
    "\n",
    "$$\n",
    "\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\sqrt{h}\\|W\\|_\\text{Frob}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\|W\\|_\\text{Frob} =  \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^h  w_{ij}^2}$\n",
    "* $\\|W\\|_\\text{Mix} = \\sqrt{\\sum_{i=1}^d \\big(\\sum_{j=1}^h | w_{ij}|\\big)^2}$\n",
    "* $\\text{Grad} = \\textstyle \\frac1N \\sum_{n=1}^N\\|\\nabla_{\\boldsymbol{x}}f (\\boldsymbol{x_n})\\|$\n",
    "\n",
    "and where $d$ is the number of input features, $h$ is the number of neurons in the hidden layer, and $W$ is the matrix of weights in the first layer (*Note that in PyTorch, the matrix of weights is given in transposed form*).\n",
    "\n",
    "As a first step, we would like to keep track of these quantities during training. The function `Frob(nn)` that computes $\\|W\\|_\\text{Frob}$ is already implemented for you.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "* Implement the function `Mix(nn)` that receives the neural network as input and returns $\\|W\\|_\\text{Mix}$.\n",
    "* Implement the function `Grad(nn,X)` that receives the neural network and some dataset as input, and computes the averaged gradient norm ($\\text{Grad}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T16:03:19.498841Z",
     "start_time": "2022-02-28T16:03:19.492779Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Frob(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    return (W**2).sum()**.5\n",
    "    \n",
    "def Mix(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    return (W.abs().sum(dim=0)**2).sum()**0.5\n",
    "    \n",
    "def Grad(nn,X):\n",
    "    X  = torch.Tensor(X)\n",
    "    X.requires_grad = True\n",
    "    nn.predict(X).sum().backward()\n",
    "    return ((X.grad**2).sum(dim=1)**0.5).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code measures these three quantities before and after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T16:03:27.806911Z",
     "start_time": "2022-02-28T16:03:20.746779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">     Before | Train Acc:  0.391 | Test Acc:  0.372 | Grad:   0.389 | WMix:   4.966 | sqrt(h)*WFrob:   5.751\n",
      ">      After | Train Acc:  1.000 | Test Acc:  0.884 | Grad:   7.297 | WMix:  40.103 | sqrt(h)*WFrob:  56.739\n"
     ]
    }
   ],
   "source": [
    "def fullpretty(name,nn):\n",
    "    return pretty(name,nn) + ' | Grad: %7.3f | WMix: %7.3f | sqrt(h)*WFrob: %7.3f'%(Grad(nn,Xtest),Mix(nn),nh**.5*Frob(nn))\n",
    "\n",
    "nnr = NeuralNetworkClassifier()\n",
    "print(fullpretty('Before',nnr))\n",
    "nnr.fit(Xtrain,Ttrain)\n",
    "print(fullpretty('After',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\sqrt{h} \\|W\\|_\\text{Frob}$ we have proven also holds empirically. We also observe that these quantities tend to increase as training proceeds. This is a typical behavior, as the network starts rather simple and becomes complex as more and more variations in the training data are being captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm Penalties (15 P)\n",
    "\n",
    "We consider the new objective $J^\\text{Frob}(\\theta) = \\text{MSE}(\\theta) + \\lambda \\cdot (\\sqrt{h} \\|W\\|_\\text{Frob})^2$, where the first term is the original mean square error objective and where the second term is the added penalty. We hardcode the penalty coeffecient to $\\lambda = 0.005$. In principle, for maximum performance and fair comparison between the methods, several of them should be tried (also for other models), and selected based on some validation set. Here, for simplicity, we omit this step.\n",
    "\n",
    "A downside of the Frobenius norm is that it is not a very tight upper bound of the gradient, that is, penalizing it is does not penalize specifically high gradient. Instead, other useful properties of the model could be negatively affected by it. Therefore, we also experiment with the mixed-norm regularizer $\\textstyle \\lambda \\cdot \\|W\\|_\\text{Mix}^2$, which is a tighter bound of the gradient, and where we also hardcode the penalty coefficient to $\\lambda = 0.025$.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* Create two new classifiers by reimplementing the regularization function with the Frobenius norm regularizer and Mixed norm regularizer respectively. You may for this task call the norm functions implemented in the question above, but this time you also need to ensure that these functions can be differentiated w.r.t. the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements and train neural networks with the new regularizers, and compares the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T16:07:46.733007Z",
     "start_time": "2022-02-28T16:07:46.727965Z"
    }
   },
   "outputs": [],
   "source": [
    "class FrobClassifier(NeuralNetworkClassifier):\n",
    "    \n",
    "    def reg(self):\n",
    "        return 0.005 * len(self.s) * Frob(self) ** 2\n",
    "    \n",
    "class MixClassifier(NeuralNetworkClassifier):\n",
    "    \n",
    "    def reg(self):\n",
    "        return 0.0025 * Mix(self) ** 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T16:08:05.425620Z",
     "start_time": "2022-02-28T16:07:48.342889Z"
    }
   },
   "outputs": [],
   "source": [
    "nnfrob = FrobClassifier()\n",
    "nnfrob.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnmix = MixClassifier()\n",
    "nnmix.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T16:08:09.903682Z",
     "start_time": "2022-02-28T16:08:09.864212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | Train Acc:  1.000 | Test Acc:  0.940\n",
      ">        SVC | Train Acc:  0.958 | Test Acc:  0.951\n",
      ">         NN | Train Acc:  1.000 | Test Acc:  0.884 | Grad:   7.297 | WMix:  40.103 | sqrt(h)*WFrob:  56.739\n",
      ">    NN+Frob | Train Acc:  0.951 | Test Acc:  0.958 | Grad:   0.806 | WMix:   1.698 | sqrt(h)*WFrob:   2.663\n",
      ">     NN+Mix | Train Acc:  0.968 | Test Acc:  0.930 | Grad:   1.564 | WMix:   4.092 | sqrt(h)*WFrob:  15.817\n"
     ]
    }
   ],
   "source": [
    "print(pretty('RForest',rfc))\n",
    "print(pretty('SVC',svc))\n",
    "print(fullpretty('NN',nnc))\n",
    "print(fullpretty('NN+Frob',nnfrob))\n",
    "print(fullpretty('NN+Mix',nnmix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the regularized neural networks perform on par with the baselines. It is interesting to observe that the mixed norm penalty more selectively reduced the gradient, and has let the Frobenius norm take higher values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
